{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Fundamentals - Solutions\n",
    "\n",
    "This notebook covers the fundamental building blocks of Deep Learning as discussed in the lecture. \n",
    "You will implement Perceptrons, Activation Functions, and specific components of Neural Networks.\n",
    "\n",
    "## Topics Covered:\n",
    "1. **Perceptrons & Logic Gates** (AND, OR, NOT)\n",
    "2. **Activation Functions** (Sigmoid, ReLU, Tanh)\n",
    "3. **Multi-Layer Perceptrons (MLP)** (Solving XOR)\n",
    "4. **Forward & Backward Propagation** (Step-by-step implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Perceptron as a Logic Gate\n",
    "\n",
    "A single Perceptron can act as a basic Logic Gate. \n",
    "\n",
    "Recall the formula for a perceptron:\n",
    "$$ Output = \\begin{cases} 1 & \\text{if } w \\cdot x + b > 0 \\\\ 0 & \\text{otherwise} \\end{cases} $$\n",
    "*(Note: In some contexts, threshold is moved to basic bias)*\n",
    "\n",
    "### Exercise 1.1: Implement Logic Gates\n",
    "Using the weights and biases (or thresholds) derived in the lecture, implement the `AND`, `OR`, and `NOT` gates.\n",
    "\n",
    "**Reference values from lecture:**\n",
    "- **AND**: $w_1=1, w_2=1, t=1.5$\n",
    "- **OR**: $w_1=1, w_2=1, t=0.5$\n",
    "- **NOT**: $w_1=-1, t=-0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x, w, t):\n",
    "    \"\"\"\n",
    "    A simple binary threshold perceptron.\n",
    "    Args:\n",
    "        x: list or array of inputs\n",
    "        w: list or array of weights\n",
    "        t: threshold value\n",
    "    Returns:\n",
    "        0 or 1\n",
    "    \"\"\"\n",
    "    # Calculate weighted sum\n",
    "    weighted_sum = np.dot(x, w)\n",
    "    \n",
    "    # Apply threshold\n",
    "    if weighted_sum >= t:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# --- AND GATE ---\n",
    "def AND_gate(x1, x2):\n",
    "    w = [1, 1]\n",
    "    t = 1.5\n",
    "    return perceptron([x1, x2], w, t)\n",
    "\n",
    "# --- OR GATE ---\n",
    "def OR_gate(x1, x2):\n",
    "    w = [1, 1]\n",
    "    t = 0.5\n",
    "    return perceptron([x1, x2], w, t)\n",
    "\n",
    "# --- NOT GATE ---\n",
    "def NOT_gate(x1):\n",
    "    w = [-1]\n",
    "    t = -0.5\n",
    "    return perceptron([x1], w, t)\n",
    "\n",
    "# Testing\n",
    "print(\"AND(0,0) =\", AND_gate(0,0))\n",
    "print(\"AND(0,1) =\", AND_gate(0,1))\n",
    "print(\"AND(1,1) =\", AND_gate(1,1))\n",
    "print(\"OR(0,1)  =\", OR_gate(0,1))\n",
    "print(\"NOT(0)   =\", NOT_gate(0))\n",
    "print(\"NOT(1)   =\", NOT_gate(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Activation Functions\n",
    "\n",
    "Neuron fire rates are rarely just 0 or 1. We use activation functions to introduce non-linearity and continuous output.\n",
    "\n",
    "### Exercise 2.1: Implement Sigmoid, Tanh, and ReLU\n",
    "\n",
    "Formulas:\n",
    "- **Sigmoid**: $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- **Tanh**: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "- **ReLU**: $f(x) = \\max(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Visualization\n",
    "x_range = np.linspace(-10, 10, 100)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x_range, sigmoid(x_range))\n",
    "plt.title(\"Sigmoid\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x_range, tanh(x_range))\n",
    "plt.title(\"Tanh\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x_range, relu(x_range))\n",
    "plt.title(\"ReLU\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The MLP & The XOR Problem\n",
    "\n",
    "A single perceptron cannot solve XOR because it is not linearly separable. We need a Multi-Layer Perceptron (MLP).\n",
    "We can build an XOR gate by combining AND, OR, and NOT gates.\n",
    "\n",
    "$$ XOR(x_1, x_2) = (x_1 \\text{ OR } x_2) \\text{ AND } (\\text{NOT } (x_1 \\text{ AND } x_2)) $$\n",
    "\n",
    "### Exercise 3.1: Implement XOR using your gates\n",
    "Combine the functions you wrote in Part 1 to solve XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR_gate(x1, x2):\n",
    "    # Layer 1\n",
    "    or_out = OR_gate(x1, x2)\n",
    "    and_out = AND_gate(x1, x2)\n",
    "    \n",
    "    # NAND part (NOT of AND)\n",
    "    nand_out = NOT_gate(and_out)\n",
    "    \n",
    "    # Layer 2 (Final AND)\n",
    "    final_out = AND_gate(or_out, nand_out)\n",
    "    return final_out\n",
    "\n",
    "# Testing XOR\n",
    "print(\"XOR(0,0) =\", XOR_gate(0,0), \"(Expected 0)\")\n",
    "print(\"XOR(0,1) =\", XOR_gate(0,1), \"(Expected 1)\")\n",
    "print(\"XOR(1,0) =\", XOR_gate(1,0), \"(Expected 1)\")\n",
    "print(\"XOR(1,1) =\", XOR_gate(1,1), \"(Expected 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training with Backpropagation (Single Step)\n",
    "\n",
    "In a real network, we don't hardcode weights. We train them using **Gradient Descent** and **Backpropagation**.\n",
    "\n",
    "Let's implement a single update step for a simple neuron with a Sigmoid activation:\n",
    "$$ \\hat{y} = \\sigma(w \\cdot x + b) $$\n",
    "$$ Loss = \\frac{1}{2}(y - \\hat{y})^2 $$\n",
    "\n",
    "### Exercise 4.1: Compute Gradients and Update Weights\n",
    "\n",
    "1. **Forward Pass**: Compute output.\n",
    "2. **Compute Error**: Difference between true $y$ and predicted $\\hat{y}$.\n",
    "3. **Backward Pass**: Compute gradient of Loss w.r.t $w$.\n",
    "   $$ \\frac{\\partial L}{\\partial w} = -(y - \\hat{y}) \\cdot \\sigma'(\\text{logit}) \\cdot x $$\n",
    "   where $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$.\n",
    "4. **Update**: $w_{new} = w_{old} - \\eta \\cdot \\text{gradient}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(output):\n",
    "    # If output is sigmoid(z), derivative is output * (1 - output)\n",
    "    return output * (1 - output)\n",
    "\n",
    "# 1. Initialize\n",
    "x = np.array([0.5, -0.2])  # Input example\n",
    "y_true = 1.0               # Correct label\n",
    "w = np.array([0.1, 0.5])   # Random weights\n",
    "b = 0.0\n",
    "learning_rate = 0.1\n",
    "\n",
    "print(f\"Initial Weights: {w}\")\n",
    "\n",
    "# 2. Forward Pass\n",
    "logit = np.dot(x, w) + b\n",
    "y_pred = sigmoid(logit)\n",
    "print(f\"Prediction: {y_pred:.4f}\")\n",
    "\n",
    "# 3. Calculate Error\n",
    "error = y_pred - y_true \n",
    "# (Note: Simple difference for derivative, though MSE loss is (y-y_hat)^2/2, derivative chain rule uses (y_pred - y_true))\n",
    "\n",
    "print(f\"Error: {error:.4f}\")\n",
    "\n",
    "# 4. Backward Pass (Gradients)\n",
    "# dL/dy_pred * dy_pred/dlogit * dlogit/dw\n",
    "# dL/dy_pred = error (if we use 1/2 MSE)\n",
    "d_output = error * sigmoid_derivative(y_pred)\n",
    "\n",
    "# Gradients for weights and bias\n",
    "grad_w = d_output * x\n",
    "grad_b = d_output * 1.0\n",
    "\n",
    "# 5. Update Weights\n",
    "w_new = w - learning_rate * grad_w\n",
    "b_new = b - learning_rate * grad_b\n",
    "\n",
    "print(f\"Updated Weights: {w_new}\")\n",
    "print(f\"Updated Bias: {b_new:.4f}\")\n",
    "\n",
    "# Verify improvement\n",
    "new_pred = sigmoid(np.dot(x, w_new) + b_new)\n",
    "print(f\"New Prediction: {new_pred:.4f} (Should be closer to {y_true})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}