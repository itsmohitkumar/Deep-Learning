{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Fundamentals - Solutions\n",
    "\n",
    "This notebook covers the fundamental building blocks of Deep Learning as discussed in the lecture. \n",
    "You will implement Perceptrons, Activation Functions, and specific components of Neural Networks.\n",
    "\n",
    "## Topics Covered:\n",
    "1. **Perceptrons & Logic Gates** (AND, OR, NOT)\n",
    "2. **Activation Functions** (Sigmoid, ReLU, Tanh)\n",
    "3. **Multi-Layer Perceptrons (MLP)** (Solving XOR)\n",
    "4. **Forward & Backward Propagation** (Step-by-step implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Perceptron as a Logic Gate\n",
    "\n",
    "A single Perceptron can act as a basic Logic Gate. \n",
    "\n",
    "Recall the formula for a perceptron:\n",
    "$$ Output = \\begin{cases} 1 & \\text{if } w \\cdot x + b > 0 \\\\ 0 & \\text{otherwise} \\end{cases} $$\n",
    "*(Note: In some contexts, threshold is moved to basic bias)*\n",
    "\n",
    "### Exercise 1.1: Implement Logic Gates\n",
    "Using the weights and biases (or thresholds) derived in the lecture, implement the `AND`, `OR`, and `NOT` gates.\n",
    "\n",
    "**Reference values from lecture:**\n",
    "- **AND**: $w_1=1, w_2=1, t=1.5$\n",
    "- **OR**: $w_1=1, w_2=1, t=0.5$\n",
    "- **NOT**: $w_1=-1, t=-0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x, w, t):\n",
    "    \"\"\"\n",
    "    A simple binary threshold perceptron.\n",
    "    Args:\n",
    "        x: list or array of inputs\n",
    "        w: list or array of weights\n",
    "        t: threshold value\n",
    "    Returns:\n",
    "        0 or 1\n",
    "    \"\"\"\n",
    "    # TODO: Implement the weighted sum and threshold check\n",
    "    pass\n",
    "\n",
    "# --- AND GATE ---\n",
    "def AND_gate(x1, x2):\n",
    "    # TODO: Set appropriate weights and threshold\n",
    "    pass\n",
    "\n",
    "# --- OR GATE ---\n",
    "def OR_gate(x1, x2):\n",
    "    pass\n",
    "\n",
    "# --- NOT GATE ---\n",
    "def NOT_gate(x1):\n",
    "    pass\n",
    "\n",
    "# Testing (Uncomment when implemented)\n",
    "# print(\"AND(0,0) =\", AND_gate(0,0))\n",
    "# print(\"AND(0,1) =\", AND_gate(0,1))\n",
    "# print(\"AND(1,1) =\", AND_gate(1,1))\n",
    "# print(\"OR(0,1)  =\", OR_gate(0,1))\n",
    "# print(\"NOT(0)   =\", NOT_gate(0))\n",
    "# print(\"NOT(1)   =\", NOT_gate(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Activation Functions\n",
    "\n",
    "Neuron fire rates are rarely just 0 or 1. We use activation functions to introduce non-linearity and continuous output.\n",
    "\n",
    "### Exercise 2.1: Implement Sigmoid, Tanh, and ReLU\n",
    "\n",
    "Formulas:\n",
    "- **Sigmoid**: $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- **Tanh**: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "- **ReLU**: $f(x) = \\max(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # TODO: Implement Sigmoid\n",
    "    pass\n",
    "\n",
    "def tanh(x):\n",
    "    # TODO: Implement Tanh\n",
    "    pass\n",
    "\n",
    "def relu(x):\n",
    "    # TODO: Implement ReLU\n",
    "    pass\n",
    "\n",
    "# Visualization\n",
    "x_range = np.linspace(-10, 10, 100)\n",
    "\n",
    "# TODO: Plot the functions once implemented\n",
    "# plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(1, 3, 1); plt.plot(x_range, sigmoid(x_range)); plt.title(\"Sigmoid\")\n",
    "# plt.subplot(1, 3, 2); plt.plot(x_range, tanh(x_range)); plt.title(\"Tanh\")\n",
    "# plt.subplot(1, 3, 3); plt.plot(x_range, relu(x_range)); plt.title(\"ReLU\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The MLP & The XOR Problem\n",
    "\n",
    "A single perceptron cannot solve XOR because it is not linearly separable. We need a Multi-Layer Perceptron (MLP).\n",
    "We can build an XOR gate by combining AND, OR, and NOT gates.\n",
    "\n",
    "$$ XOR(x_1, x_2) = (x_1 \\text{ OR } x_2) \\text{ AND } (\\text{NOT } (x_1 \\text{ AND } x_2)) $$\n",
    "\n",
    "### Exercise 3.1: Implement XOR using your gates\n",
    "Combine the functions you wrote in Part 1 to solve XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR_gate(x1, x2):\n",
    "    # TODO: Combine AND, OR, and NOT gates to implement XOR\n",
    "    # Hint: XOR(x1, x2) = (x1 OR x2) AND (NOT (x1 AND x2))\n",
    "    pass\n",
    "\n",
    "# Testing XOR\n",
    "# print(\"XOR(0,0) =\", XOR_gate(0,0))\n",
    "# print(\"XOR(0,1) =\", XOR_gate(0,1))\n",
    "# print(\"XOR(1,0) =\", XOR_gate(1,0))\n",
    "# print(\"XOR(1,1) =\", XOR_gate(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training with Backpropagation (Single Step)\n",
    "\n",
    "In a real network, we don't hardcode weights. We train them using **Gradient Descent** and **Backpropagation**.\n",
    "\n",
    "Let's implement a single update step for a simple neuron with a Sigmoid activation:\n",
    "$$ \\hat{y} = \\sigma(w \\cdot x + b) $$\n",
    "$$ Loss = \\frac{1}{2}(y - \\hat{y})^2 $$\n",
    "\n",
    "### Exercise 4.1: Compute Gradients and Update Weights\n",
    "\n",
    "1. **Forward Pass**: Compute output.\n",
    "2. **Compute Error**: Difference between true $y$ and predicted $\\hat{y}$.\n",
    "3. **Backward Pass**: Compute gradient of Loss w.r.t $w$.\n",
    "   $$ \\frac{\\partial L}{\\partial w} = -(y - \\hat{y}) \\cdot \\sigma'(\\text{logit}) \\cdot x $$\n",
    "   where $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$.\n",
    "4. **Update**: $w_{new} = w_{old} - \\eta \\cdot \\text{gradient}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(output):\n",
    "    # TODO: Implement derivative of sigmoid: f'(x) = f(x)(1-f(x))\n",
    "    pass\n",
    "\n",
    "# 1. Initialize\n",
    "x = np.array([0.5, -0.2])\n",
    "y_true = 1.0\n",
    "w = np.array([0.1, 0.5])\n",
    "b = 0.0\n",
    "learning_rate = 0.1\n",
    "\n",
    "print(f\"Initial Weights: {w}\")\n",
    "\n",
    "# 2. Forward Pass\n",
    "# TODO: Compute logit and prediction\n",
    "y_pred = 0\n",
    "print(f\"Prediction: {y_pred}\")\n",
    "\n",
    "# 3. Calculate Error\n",
    "error = 0 # TODO\n",
    "print(f\"Error: {error}\")\n",
    "\n",
    "# 4. Backward Pass\n",
    "# TODO: Calculate gradients\n",
    "grad_w = 0\n",
    "grad_b = 0\n",
    "\n",
    "# 5. Update Weights\n",
    "# TODO: Update w and b\n",
    "w_new = w\n",
    "b_new = b\n",
    "\n",
    "print(f\"Updated Weights: {w_new}\")\n",
    "print(f\"Updated Bias: {b_new}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}