{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# NLP Tutorial: From Text to Vectors\n",
                "\n",
                "This tutorial covers the complete flow of Natural Language Processing (NLP), broken into manageable steps:\n",
                "1.  **Data Preprocessing**\n",
                "2.  **Data Cleaning**\n",
                "3.  **Text to Vectors** (including Word Embeddings: CBOW & Skipgram)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1. Data Preprocessing"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.1 Tokenization\n",
                "Tokenization splits text into individual units like words or sentences. It is the fundamental first step in turning unstructured text into structured data.\n",
                "\n",
                "First, we import the necessary libraries and download the required NLTK data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize, word_tokenize\n",
                "\n",
                "# Download necessary tokenizer data\n",
                "nltk.download('punkt')\n",
                "nltk.download('punkt_tab')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, let's define a sample text to work with."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\"\n",
                "print(text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Sentence Tokenization\n",
                "Split the text into individual sentences."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sentences = sent_tokenize(text)\n",
                "print(\"--- Sentences ---\")\n",
                "for sent in sentences:\n",
                "    print(sent)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Word Tokenization\n",
                "Split the text into individual words."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "words = word_tokenize(text)\n",
                "print(\"--- Words ---\")\n",
                "print(words)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.2 Lowercasing\n",
                "Lowercasing normalizes text to ensure that words like 'Apple' and 'apple' are treated as identical. This reduces the vocabulary size and complexity."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_lower = text.lower()\n",
                "print(\"Original :\", text)\n",
                "print(\"Lowercased:\", text_lower)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.3 Regular Expressions (Regex)\n",
                "Regular Expressions (Regex) allow for pattern-based text searching and manipulation. They are essential for removing noise like HTML tags, URLs, or special characters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "dirty_text = \"Check out this link <a href='test'>Click</a> call 999-999-9999 or email test@example.com!!! #NLP\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Remove HTML Tags\n",
                "We use the pattern `<.*?>` to find and remove HTML tags."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "clean_text = re.sub('<.*?>', '', dirty_text)\n",
                "print(\"No HTML:\", clean_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Remove Special Characters\n",
                "We keep only letters and spaces using the pattern `[^a-zA-Z\\s]`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "clean_text = re.sub('[^a-zA-Z\\s]', '', clean_text)\n",
                "print(\"Only Letters:\", clean_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2. Data Cleaning"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.1 Stemming\n",
                "Stemming is a crude heuristic process that chops off word endings to reduce them to a base form. It is fast but often results in non-dictionary roots (e.g., 'flies' -> 'fli').\n",
                "\n",
                "Initializing stemmers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from nltk.stem import PorterStemmer, LancasterStemmer\n",
                "\n",
                "porter = PorterStemmer()\n",
                "lancaster = LancasterStemmer()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Comparing Stemmer results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "word = \"history\"\n",
                "print(f\"Porter:    {porter.stem(word)}\")\n",
                "print(f\"Lancaster: {lancaster.stem(word)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 Lemmatization\n",
                "Lemmatization uses a vocabulary and morphological analysis to return the dictionary form of a word. Unlike stemming, it produces valid words but is computationally more expensive.\n",
                "\n",
                "Using WordNetLemmatizer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nltk.download('wordnet')\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "\n",
                "lemmatizer = WordNetLemmatizer()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Comparing Lemmatization with Stemming for the word 'eating' (verb)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "word = \"eating\"\n",
                "print(f\"Stem:  {porter.stem(word)}\")\n",
                "print(f\"Lemma: {lemmatizer.lemmatize(word, pos='v')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.3 Stopwords\n",
                "Stopwords are high-frequency words (like 'the', 'is', 'and') that carry little semantic meaning. Removing them helps the model focus on the unique, content-rich words."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nltk.download('stopwords')\n",
                "from nltk.corpus import stopwords\n",
                "\n",
                "stop_words = set(stopwords.words('english'))\n",
                "print(f\"Total English stopwords: {len(stop_words)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Applying stopword removal."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_sample = \"This is a sample sentence, showing off the stop words filtration.\"\n",
                "words = word_tokenize(text_sample)\n",
                "\n",
                "filtered_sentence = [w for w in words if not w.lower() in stop_words]\n",
                "\n",
                "print(\"Original:\", words)\n",
                "print(\"Filtered:\", filtered_sentence)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. Text to Vectors"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.1 One Hot Encoding\n",
                "One Hot Encoding creates a binary vector for each word, where only one bit is true. It is simple but results in high-dimensional, sparse vectors with no semantic relation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "docs = [\"blue house\", \"red house\"]\n",
                "print(\"Docs:\", docs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "one_hot = pd.get_dummies(docs[0].split() + docs[1].split())\n",
                "print(one_hot)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.2 Bag of Words (BoW)\n",
                "Bag of Words represents a document by counting the frequency of each word it contains. It captures word presence but ignores grammar and word order."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "\n",
                "corpus = [\n",
                "    'This is the first document.',\n",
                "    'This document is the second document.',\n",
                "    'And this is the third one.',\n",
                "    'Is this the first document?',\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer = CountVectorizer()\n",
                "X = vectorizer.fit_transform(corpus)\n",
                "\n",
                "print(\"Vocabulary:\", vectorizer.get_feature_names_out())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"BoW Matrix:\\n\", X.toarray())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.3 TF-IDF\n",
                "TF-IDF (Term Frequency-Inverse Document Frequency) weighs words by their importance. It helps to deemphasize common words and highlight terms that are unique to specific documents."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                "tfidf_vectorizer = TfidfVectorizer()\n",
                "X_tfidf = tfidf_vectorizer.fit_transform(corpus)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"TF-IDF Matrix:\\n\", X_tfidf.toarray())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.4 Word Embeddings (Word2Vec)\n",
                "Word Embeddings are dense vector representations where words with similar meanings are located close together. They capture complex semantic relationships that sparse methods miss."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install gensim\n",
                "from gensim.models import Word2Vec\n",
                "\n",
                "# Sample Data\n",
                "sentences = [\n",
                "    ['i', 'love', 'nlp'],\n",
                "    ['nlp', 'is', 'awesome'],\n",
                "    ['i', 'love', 'machine', 'learning'],\n",
                "    ['deep', 'learning', 'is', 'a', 'subset', 'of', 'machine', 'learning'],\n",
                "    ['word', 'embeddings', 'are', 'dense', 'vectors']\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4.1 CBOW (Continuous Bag of Words)\n",
                "CBOW predicts the *center* word based on the surrounding *context* words. It is faster to train and has slightly better accuracy for frequent words."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train CBOW Model (sg=0)\n",
                "model_cbow = Word2Vec(sentences, min_count=1, vector_size=10, window=3, sg=0)\n",
                "print(\"CBOW Model Trained.\")\n",
                "\n",
                "vector_cbow = model_cbow.wv['nlp']\n",
                "print(\"CBOW Vector for 'nlp':\\n\", vector_cbow)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4.2 Skipgram\n",
                "Skipgram predicts the surrounding *context* words given a *center* word. It performs well with small datasets and handles infrequent words better than CBOW."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Skipgram Model (sg=1)\n",
                "model_skipgram = Word2Vec(sentences, min_count=1, vector_size=10, window=3, sg=1)\n",
                "print(\"Skipgram Model Trained.\")\n",
                "\n",
                "vector_skipgram = model_skipgram.wv['nlp']\n",
                "print(\"Skipgram Vector for 'nlp':\\n\", vector_skipgram)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.5 Average Word2Vec\n",
                "Create a document vector by averaging word vectors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "def avg_word2vec(sentence, model):\n",
                "    words = sentence.split()\n",
                "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
                "    \n",
                "    if not word_vectors:\n",
                "        return np.zeros(model.vector_size)\n",
                "    \n",
                "    return np.mean(word_vectors, axis=0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Testing Average Word2Vec with the CBOW model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "new_sentence = \"i love deep learning\"\n",
                "doc_vector = avg_word2vec(new_sentence, model_cbow)\n",
                "\n",
                "print(f\"Document Vector (CBOW) for '{new_sentence}':\\n\", doc_vector)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}